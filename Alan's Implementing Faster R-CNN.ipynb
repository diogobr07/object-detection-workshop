{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Faster R-CNN\n",
    "\n",
    "The objective of this activity is to implement the main parts of the Faster R-CNN algorithm. We'll use pre-trained weights to guide and facilitate the process, and implement all the stages one by one.\n",
    "\n",
    "We've tried to keep code in the notebooks to a minimum, mainly data manipulation and visualization, to make it easy enough to follow. All accompanying code is under the `workshop` Python package.\n",
    "\n",
    "After some introductory code, the notebook will continue as follows:\n",
    "* Playing with a **pre-trained Resnet** to obtain features out of an image.\n",
    "* Generate regions of interest by implementing the **Region Proposal Network** detailed in [1].\n",
    "* Prepare this regions to be fed to the second stage, by applying **RoI pooling**.\n",
    "* Classify and refine said networks by passing them through an **R-CNN**, as detailed in [2].\n",
    "\n",
    "We'll present you with stubs for the different functions required and your task will be to fill them in.\n",
    "\n",
    "Note that, due to time constraints, we won't actually implement anything relating to training the model itself. We'll be focusing on the required machinery for inference, and give some pointers on what's missing to train it from scratch.\n",
    "\n",
    "* [1] Ren, Shaoqing, et al. *Faster R-CNN: Towards real-time object detection with region proposal networks.*\n",
    "* [2] Girshick, Ross. *Fast R-CNN.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The basics\n",
    "We'll start with some imports.\n",
    "\n",
    "The local imports are under the `workshop` package, which you should have installed using `pip install -e workshop/` in the virtualenv you're running your notebook on. [Remove if we're going to use Azure.]\n",
    "\n",
    "Within `workshop` we have some modules:\n",
    "* `vis`: various visualization utilities to draw bounding boxes, sliders, etc.\n",
    "* `image`: utilities for reading images and loading them into PIL (the imaging library).\n",
    "* `resnet`: the implementation for the base network we're going to use (more on this shortly).\n",
    "* `faster`: utilities and parts we won't be implementing but provide for completeness' sake.\n",
    "\n",
    "Let's test some things to make sure everything is up and ready to go.\n",
    "\n",
    "Start by running the following in your terminal, and then test the rest of the imports:\n",
    "```bash\n",
    " $ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, IntSlider, Layout\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Try to enable TF eager execution, or do nothing if running again.\n",
    "try:\n",
    "    tf.enable_eager_execution()\n",
    "except ValueError:\n",
    "    # Already executed.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Local imports.\n",
    "from workshop.faster import run_base_network, clip_boxes, get_width_upright, rcnn_proposals\n",
    "from workshop.image import open_all_images, open_image, to_image\n",
    "from workshop.resnet import resnet_v1_101, resnet_v1_101_tail\n",
    "from workshop.vis import image_grid, draw_bboxes, draw_bboxes_with_labels\n",
    "\n",
    "# Notebook-specific settings.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now load some images to play with and display them below. Change which image is passed to the `to_image` function to see it in full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = open_all_images('images/')\n",
    "\n",
    "axes = image_grid(len(images))\n",
    "for ax, (name, image) in zip(axes, images.items()):\n",
    "    ax.imshow(np.squeeze(image))\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.subplots_adjust(wspace=.01)\n",
    "plt.show()\n",
    "\n",
    "image = images['bicycles']\n",
    "\n",
    "# `to_image` turns a `numpy.ndarray` into a PIL image, so it's displayed by the notebook.\n",
    "to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# The base network: ResNet\n",
    "\n",
    "\n",
    "The basis for the Faster R-CNN algorithm is to leverage a pre-trained classifier network to extract feature maps (also called *activation maps*) from the image. For this implementation, we'll be using the popular ResNet 101 [3].\n",
    "\n",
    "We provide the implementation itself (which you can see in the `workshop.resnet` module), as well as a checkpoint with the weights (in the `checkpoint/` directory).\n",
    "\n",
    "Run the base network on different images, in order to see how the different feature maps behave. **Can you notice any particular features being detected in the activation maps?**\n",
    "\n",
    "* [3] He, Kaiming, et al. *Deep residual learning for image recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    feature_map = run_base_network(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_page = 20\n",
    "\n",
    "slider = IntSlider(\n",
    "    min=0, max=1024 // per_page, value=0,\n",
    "    description='Feature map',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(page=slider)\n",
    "def display_feature_maps(page):\n",
    "    axes = image_grid(per_page)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(\n",
    "            feature_map.numpy()[0, :, :, page * per_page + idx],\n",
    "            cmap='gray', aspect='auto'\n",
    "        )\n",
    "\n",
    "    plt.subplots_adjust(wspace=.01, hspace=.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay the feature maps into the image themselves, so we can take a more detailed look into what feature of the image the ResNet reacts to.\n",
    "\n",
    "See, for example:\n",
    "* Feature maps 19, 22 in `cats`.\n",
    "* Feature map 34, 64 in `bicycles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider = IntSlider(\n",
    "    min=0, max=1024, value=0,\n",
    "    description='Feature map index',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(idx=slider)\n",
    "def overlay_feature_map(idx=0):\n",
    "    # Normalize the feature map so we get the whole range of colors.\n",
    "    fm = (\n",
    "        feature_map.numpy()[0, :, :, idx]\n",
    "        / feature_map.numpy()[0, :, :, idx].max()\n",
    "        * 255\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "    fm_image = fm_image.resize(\n",
    "        image.shape[1:3][::-1],\n",
    "        resample=Image.NEAREST\n",
    "    )\n",
    "    \n",
    "    # Add some alpha to overlay it over the image.\n",
    "    fm_image.putalpha(200)\n",
    "    \n",
    "    base_image = to_image(image)\n",
    "    base_image.paste(fm_image, (0, 0), fm_image)\n",
    "    \n",
    "    return base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section didn't require any implementation at all, but get ready, because we're about to. The main idea here was illustrating what we mean when we say that the later layers of a classification network are **feature detectors**, reacting to particular structures in an image.\n",
    "\n",
    "What would you do if you were to use this information to detect objects? How could you leverage the fact that we can say \"there's a cat ear here!\"? We're going to explore these questions in the following sections.\n",
    "\n",
    "For now, back to the slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Finding stuff with the Region Proposal Network\n",
    "\n",
    "Having gone through the theory, we'll now turn our attention to implementing a **Region Proposal Network**. The idea, as we've seen, is to use the feature maps provided by the ResNet to find out **where** there might be an object located.\n",
    "\n",
    "This is where **anchors** come into play. We'll take a grid of points over the image and consider several (15, in this case) anchors, or reference boxes, for each of them. The RPN layers themselves will then predict whether there's an object in each of these 15 boxes **and** how much we need to resize them to better fit it.\n",
    "\n",
    "The tasks we have ahead of us are, thus:\n",
    "* Get the **coordinates** $(x_{min}, y_{min}, x_{max}, y_{max})$ for each of the anchors. There are $15$ anchors and the centers will be separated by approximately $16$ pixels, so we're talking about several thousands of coordinates.\n",
    "* Find out how to do the special **encoding and decoding** of coordinates described in Faster R-CNN so the RPN can predict locations in the image.\n",
    "* Build the **convolutional layers** comprising the RPN and run them through different images.\n",
    "* **Translate the predictions** of the RPN layer into usable proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating anchors\n",
    "\n",
    "We'll get the anchor's coordinates in two steps. First we'll implement the `generate_anchors_reference` function which will return, given the anchors' settings (i.e. size, aspect ratio, scales), the coordinates for said boxes (in pixel space) assuming they're centered around (0, 0). This will give us, effectively, a $(15, 4)$ array.\n",
    "\n",
    "Then, we'll sum those coordinates to each of the **anchor centers** of the image in the function `generate_anchors`. Given that we're using a ResNet 101, which has a downsampling factor of 16 (i.e. every point in the final feature map corresponds to a $16\\times16$ region of the original image), we'll select the centers every 16 pixels in each direction.\n",
    "\n",
    "Go on and implement `generate_anchors_reference`, and check that the output makes sense. You can try varying the anchor settings to see if it still makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors_reference(base_size, aspect_ratios, scales):\n",
    "    \"\"\"Generate base set of anchors to be used as reference for all anchors.\n",
    "\n",
    "    Anchors vary only in width and height. Using the base_size and the\n",
    "    different ratios we can calculate the desired widths and heights.\n",
    "\n",
    "    Aspect ratios maintain the area of the anchors, while scales apply to the\n",
    "    length of it (and thus affect it squared).\n",
    "\n",
    "    Args:\n",
    "        base_size (int): Base size of the base anchor (square).\n",
    "        aspect_ratios: Ratios to use to generate different anchors. The ratio\n",
    "            is the value of height / width.\n",
    "        scales: Scaling ratios applied to length.\n",
    "\n",
    "    Returns:\n",
    "        anchors: Numpy array with shape (total_aspect_ratios * total_scales, 4)\n",
    "            with the corner points of the reference base anchors using the\n",
    "            convention (x_min, y_min, x_max, y_max).\n",
    "    \"\"\"\n",
    "    \n",
    "    # See: np.meshgrid, np.stack (no son imprescindibles).\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    return anchors\n",
    "\n",
    "\n",
    "references = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [0.5, 1, 2],  # Aspect ratios.\n",
    "    [0.125, 0.25, 0.5, 1, 2],  # Scales.\n",
    ")\n",
    "\n",
    "print('Anchor references (real image size):')\n",
    "print()\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to check if the implementation is correct is just drawing them and making sure the result makes sense. If the image is too chaotic, you can try decreasing the number of scales and aspect ratios, but do revert it back before continuing.\n",
    "\n",
    "Remember: for scales, length is doubled, for aspect ratios, the area is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "for ref_idx in range(references.shape[0]):\n",
    "    x_min, y_min, x_max, y_max = references[ref_idx, :]\n",
    "    rect = matplotlib.patches.Rectangle(\n",
    "        (x_min, -y_max),\n",
    "        x_max - x_min,\n",
    "        y_max - y_min,\n",
    "        linewidth=1,\n",
    "        edgecolor='r',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim([-500, 500])\n",
    "    ax.set_ylim([-500, 500])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for reference, let's draw one over `image`, to see how they match. Since the anchor references are centered around $(0, 0)$, we can sum $P = (x_p, y_p)$ to get the references at $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Fix/clarify y-coordinate issues.\n",
    "point = np.array([400, -270])\n",
    "\n",
    "p_min, p_max = references[:, :2], references[:, 2:]\n",
    "\n",
    "p_references = np.concatenate([point + p_min, point + p_max], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for ref_idx in range(p_references.shape[0]):\n",
    "    x_min, y_min, x_max, y_max = p_references[ref_idx, :]\n",
    "    rect = matplotlib.patches.Rectangle(\n",
    "        (x_min, -y_max),\n",
    "        x_max - x_min,\n",
    "        y_max - y_min,\n",
    "        linewidth=1,\n",
    "        edgecolor='r',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Plot the reference point in use.\n",
    "ax.plot(point[0], -point[1], marker='s', color='red', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the larger boxes cover quite a bit of the image, while the smaller ones will be useful for detecting very small objects.\n",
    "\n",
    "Now, as we said before, we want the above for all **anchor centers**, which we said were going to be located every 16 pixels. For reference, the anchor centers are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See where this should be set. Is actually dependent on the base network.\n",
    "OUTPUT_STRIDE = 16\n",
    "\n",
    "\n",
    "# Print the anchor centers in use.\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for x in range(0, image.shape[2], OUTPUT_STRIDE):\n",
    "    for y in range(0, image.shape[1], OUTPUT_STRIDE):\n",
    "        ax.plot(x, y, marker='s', color='red', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap up this part by getting the entire list of anchors for the image. This will be done within te `generate_anchors` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Document.\n",
    "ANCHOR_BASE_SIZE = 256\n",
    "ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "ANCHOR_SCALES = [0.125, 0.25, 0.5, 1, 2]\n",
    "\n",
    "\n",
    "def generate_anchors(feature_map_shape):\n",
    "    \"\"\"Generate anchors for an image.\n",
    "\n",
    "    Using the feature map (the output of the pretrained network for an image)\n",
    "    and the anchor references (generated using the specified anchor sizes and\n",
    "    ratios), we generate a list of anchors.\n",
    "\n",
    "    Anchors are just fixed bounding boxes of different ratios and sizes that\n",
    "    are uniformly generated throught the image.\n",
    "\n",
    "    Args:\n",
    "        feature_map_shape: Shape of the convolutional feature map used as\n",
    "            input for the RPN. Should be (batch, height, width, depth).\n",
    "\n",
    "    Returns:\n",
    "        all_anchors: A flattened Tensor with all the anchors of shape\n",
    "            `(num_anchors_per_points * feature_width * feature_height, 4)`\n",
    "            using the (x1, y1, x2, y2) convention.\n",
    "    \"\"\"\n",
    "\n",
    "    anchor_reference = generate_anchors_reference(\n",
    "        ANCHOR_BASE_SIZE, ANCHOR_RATIOS, ANCHOR_SCALES\n",
    "    )\n",
    "    \n",
    "    # See tf.meshgrid, tf.range, tf.expand_dims.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "\n",
    "    ####\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "anchors = generate_anchors(feature_map.shape)\n",
    "\n",
    "print('Anchors (real image size):')\n",
    "print()\n",
    "print(anchors.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get three or four anchors to corroborate that the results makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we've finished generating the anchors that will be used by the RPN. This is, effectively, a list of $15 \\times F_x \\times F_y$, where $F_x, F_y$ are the feature map width and height, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Encoding and decoding coordinates\n",
    "\n",
    "\n",
    "Deep neural networks usually train and converge better when their outputs have zero mean and unit variance (and/or their intermediate values do so). Due to this, and the difficulty in predicting values in a possibly unbounded region (pixel coordinates), a special encoding is applied to the coordinates before passing them in to the network (and after getting them out).\n",
    "\n",
    "The idea behind the encoding is to express the coordinates of a bounding box $B$ as a set of four numbers $(D_x, D_y, D_w, D_h)$ (the **deltas**) and a reference bounding box $R$. $D_x$ and $D_y$ indicate how much the center of $R$ should be moved to reach the center of $B$, normalized by the size of $B$, while $D_w$ and $D_h$ indicate how much the width and height of $B$ must be increased or decreased to reach the size of $B$ (it's actually the log of that value, but we'll get into it).\n",
    "\n",
    "We'll implement two functions here, `encode` and `decode`. While only the latter will be used, it's useful to implement both in order to understand the whole process and to make it easier to test.\n",
    "\n",
    "[leer paper de esta sección]\n",
    "[reescribir un poco esta parte]\n",
    "\n",
    "Formulas (poner paper fragment?):\n",
    "\n",
    "(Ver paper Faster-RCNN, sección 3.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might find the `get_width_upright` function useful to obtain the \n",
    "# [complete]\n",
    "\n",
    "get_width_upright?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def encode(bboxes, references):\n",
    "    \"\"\"Encode bounding boxes as deltas w.r.t. reference boxes.\n",
    "\n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (total_proposals, 4). Having the bbox\n",
    "            encoding in the (x_min, y_min, x_max, y_max) order.\n",
    "        references: Tensor of shape (total_proposals, 4). With the same bbox\n",
    "            encoding.\n",
    "\n",
    "    Returns:\n",
    "        targets: Tensor of shape (total_proposals, 4) with the different\n",
    "            deltas needed to transform the proposal to `references`. These\n",
    "            deltas are with regards to the center, width and height of the\n",
    "            two boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Shouldn't be called gt_boxes but \"references\".\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "# Test encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(roi, deltas):\n",
    "    \"\"\"\n",
    "    TODO: Docstring plus param name change.\n",
    "    \"\"\"        \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "\n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "    \n",
    "    \n",
    "# Test decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test roundtrip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Convolutional layers\n",
    "\n",
    "We now have a variable-size feature map (with a factor of 16 times smaller than the original image) and we want to predict, for each of them, how to modify (i.e. the $4$ values from above, $D_{x, y, w, h}$) each of the $k = 15$ anchors. In this context, it makes sense, then, to use a convolutional layer (or more) on the feature map, where the final number of filters will be $4 \\times k$.\n",
    "\n",
    "For each of these anchors we'll also want to decide whether we think there's an object present on said region or not (thus, $2 \\times k$ more filters). This will, in essence, look at the activation maps we saw before and decide whether, in a given region, the activated features amount to an object being in there (e.g. many cat ear features have activated).\n",
    "\n",
    "As we saw in the slides, the RPN first has a $3\\times3$ convolutional layer with $512$ filters and then two outputs heads:\n",
    "* One with $2 \\times k$ filters for the **objectness score**.\n",
    "* One with $4 \\times k$ filters for the **encoded deltas**.\n",
    "\n",
    "Both will be implemented as $1 \\times 1$ convolutions in order to support variable-size images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that when implementing Faster R-CNN for training, we should\n",
    "# also specify initializers and regularizers for the weights. We're\n",
    "# omitting them here for brevity.\n",
    "\n",
    "def build_rpn(feature_map):\n",
    "    \"\"\"Run the RPN layers through the feature map.\n",
    "    \n",
    "    Will run the input through an initial convolutional layer of\n",
    "    filter size 3x3 and 512 channels, using the ReLU6 activation.\n",
    "    The output of this layer has the same spatial size as the\n",
    "    input.\n",
    "    \n",
    "    Then run two 1x1 convolutions over this intermediate layer, one\n",
    "    for the resizings and one for the objectness probabilities.\n",
    "    Remember to apply the softmax activation over the objectness\n",
    "    scores to get a distribution.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map: Tensor of shape (1, W, H, C), with WxH the\n",
    "            spatial shape of the feature map and C the number of\n",
    "            channels (1024 in this case).\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors: the first being the output of the bbox\n",
    "        resizings `(W * H * num_anchors, 4)` and the second being\n",
    "        the objectness score, of size `(W * H * num_anchors, 2)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note: see the functions `tf.layers.conv2d` and `tf.reshape`. Also,\n",
    "    # read the docstring thoroughly to help you pass the correct\n",
    "    # parameters to the conv layers.\n",
    "    \n",
    "    # The names of the layers should be: `rpn/conv` for the base layer,\n",
    "    # `rpn/cls_conv` for the objectness score, and `rpn/bbox_conv` for\n",
    "    # the bbox resizing.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return rpn_bbox_pred, rpn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    rpn_bbox_pred, rpn_cls_prob = build_rpn(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a histogram of the bounding box modifications for our current image.\n",
    "\n",
    "Look at the results. Do they make sense? Does it seem that the encoding is indeed helping unbias the predictions? What do values near zero mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rpn_bbox_pred.numpy()\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    title = ['x_min', 'y_min', 'x_max', 'y_max'][idx]\n",
    "    ax.set_title(title)\n",
    "    ax.hist(preds[:, idx], bins=50)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the objectness scores. As you'll see, most of the anchors are deemed not worthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rpn_cls_prob.numpy()[:, 1]\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "ax.set_title('Scores (0 = no object, 1 = object)')\n",
    "ax.hist(preds, bins=100)\n",
    "\n",
    "print('{} predictions over 0.9, out of a total of {}'.format(\n",
    "    len(np.flatnonzero(preds > 0.9)), len(preds)\n",
    "))\n",
    "print()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, it may prove insightful to analyze other statistics, such as the objectness and/or resizing by anchor size, or by position in the image. Performing an analysis like this can help pick hyperparameters, guide improvements for the algorithms and find pathologies on the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generating and filtering proposals\n",
    "\n",
    "We have the output, now let's do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate proposals from the RPN's output by decoding the bounding boxes\n",
    "# according to the configured anchors.\n",
    "proposals = decode(anchors, rpn_bbox_pred)\n",
    "\n",
    "# Get the (positive-object) scores from the RPN.\n",
    "# TODO: Could be just one dimension, right? (If not for compatibility.)\n",
    "scores = tf.reshape(rpn_cls_prob[:, 1], [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = proposals.numpy()\n",
    "areas = (props[:, 2] - props[:, 0]) * (props[:, 3] - props[:, 1])\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "ax.set_title('Area per proposal')\n",
    "ax.hist(areas, bins=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Proposals with areas under zero:')\n",
    "print(np.flatnonzero(areas <= 0))\n",
    "\n",
    "# TODO: Why isn't there any?\n",
    "# Note: ~250k is 250x100, the average size of the cats here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter proposals, keeping only valid bboxes.\n",
    "\n",
    "\n",
    "def filter_proposals(proposals, scores):\n",
    "    \"\"\"Filters zero-area proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals:\n",
    "        scores:\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with zero-area proposals removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.greater`, `tf.maximum`, `tf.boolean_mask`.\n",
    "    \n",
    "    # NOTA: Ver por qué no hay áreas negativas, no tiene sentido implementar ahora mismo.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "\n",
    "# Filter proposals with negative areas.\n",
    "proposals, scores = filter_proposals(proposals, scores)\n",
    "\n",
    "# TODO: Visualize average area before (above) & after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRE_NMS_TOP_N = 12000\n",
    "POST_NMS_TOP_N = 2000\n",
    "NMS_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "# Reduce the number of proposals by applying non-maximum suppression to proposals.\n",
    "\n",
    "\n",
    "# TODO: Move this to `workshop.faster`? Or make them implement it?\n",
    "def change_order(bboxes):\n",
    "    \"\"\"Change bounding box encoding order.\n",
    "\n",
    "    TensorFlow works with the (y_min, x_min, y_max, x_max) order while we work\n",
    "    with the (x_min, y_min, x_max, y_min).\n",
    "\n",
    "    While both encoding options have its advantages and disadvantages we\n",
    "    decided to use the (x_min, y_min, x_max, y_min), forcing use to switch to\n",
    "    TensorFlow's every time we want to use a std function that handles bounding\n",
    "    boxes.\n",
    "\n",
    "    Args:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4)\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4) with the order swaped.\n",
    "    \"\"\"\n",
    "    first_min, second_min, first_max, second_max = tf.unstack(\n",
    "        bboxes, axis=1\n",
    "    )\n",
    "    bboxes = tf.stack(\n",
    "        [second_min, first_min, second_max, first_max], axis=1\n",
    "    )\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def apply_nms(proposals, scores):\n",
    "    \"\"\"Applies non-maximum suppression to proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals:\n",
    "        scores:\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with NMS applied, and ordered by score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note: See `tf.image.non_max_suppression`, `change_order`, `tf.gather`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "\n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "\n",
    "proposals, scores = apply_nms(proposals, scores)\n",
    "\n",
    "# TODO: Examples of stuff merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What have we detected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(x=(1, 40))\n",
    "def draw(x=10):\n",
    "    return draw_bboxes(image, proposals[:x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Region of Interest Pooling\n",
    "\n",
    "We've got proposals. Let's get them to a standard size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_bboxes(proposals, im_shape):\n",
    "    \"\"\"\n",
    "    Gets normalized coordinates for RoIs (between 0 and 1 for cropping)\n",
    "    in TensorFlow's order (y1, x1, y2, x2).\n",
    "\n",
    "    Args:\n",
    "        roi_proposals: A Tensor with the bounding boxes of shape\n",
    "            (total_proposals, 4), where the values for each proposal are\n",
    "            (x_min, y_min, x_max, y_max).\n",
    "        im_shape: A Tensor with the shape of the image (height, width).\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor with normalized bounding boxes in TensorFlow's\n",
    "            format order. Its should is (total_proposals, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.unstack`, `tf.stack`, `tf.cast`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "\n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "\n",
    "def roi_pooling(feature_map, proposals, im_shape, pool_size=7):\n",
    "    \"\"\"Perform RoI pooling.\n",
    "\n",
    "    This is a simplified method than what's done in the paper that obtains\n",
    "    similar results. We crop the proposal over the feature map and resize it\n",
    "    bilinearly.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map:\n",
    "        proposals:\n",
    "        im_shape:\n",
    "        pool_size:\n",
    "    \n",
    "    Returns:\n",
    "        Pooled feature map, with size `(num_proposals, pool_size, pool_size,\n",
    "        feature_map_channels)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.image.crop_and_resize`, `tf.nn.max_pool`. Probably need to\n",
    "    # give more indications than this. See the `normalize_bboxes` you\n",
    "    # implemented above.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return pooled\n",
    "\n",
    "\n",
    "pooled = roi_pooling(feature_map, proposals, (image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our pooled proposals, along with the image patches they come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_SUBPLOTS = (5, 5)\n",
    "\n",
    "pool = pooled.numpy()\n",
    "\n",
    "# Pool the images too to visualize.\n",
    "image_crops = roi_pooling(\n",
    "    image, proposals,\n",
    "    (image.shape[1], image.shape[2]),\n",
    "    pool_size=140\n",
    ").numpy().astype(np.uint8)\n",
    "\n",
    "fm_slider = IntSlider(\n",
    "    min=0, max=pool.shape[-1] - 1, description='Feature map index',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "im_slider = IntSlider(\n",
    "    min=0, max=(pool.shape[0] // 25), description='Image index',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(fm_idx=fm_slider, im_idx=im_slider)\n",
    "def display_pooled_proposal(fm_idx=0, im_idx=0):\n",
    "    figsize = (NUM_SUBPLOTS[1] * 3, NUM_SUBPLOTS[0] * 3)\n",
    "    fig, axes = plt.subplots(*NUM_SUBPLOTS, figsize=figsize)\n",
    "    \n",
    "    for idx, ax in enumerate(axes.ravel()):\n",
    "        fm = (\n",
    "            pool[idx, :, :, fm_idx]\n",
    "            / pool[idx, :, :, fm_idx].max()\n",
    "            * 255\n",
    "        ).astype(np.uint8)\n",
    "        \n",
    "        img = image_crops[im_idx * 25 + idx, :, :, :]\n",
    "        \n",
    "        fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "        fm_image = fm_image.resize(\n",
    "            img.shape[0:2][::-1],\n",
    "            resample=Image.NEAREST\n",
    "        )\n",
    "\n",
    "        # Add some alpha to overlay it over the image.\n",
    "        fm_image.putalpha(180)\n",
    "\n",
    "        base_image = Image.fromarray(img)\n",
    "        base_image.paste(fm_image, (0, 0), fm_image)\n",
    "        \n",
    "        ax.imshow(base_image, aspect='auto')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the proposals: R-CNN\n",
    "\n",
    "Now let's build a fixed-length input CNN to classify the proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Do differently.\n",
    "# TODO: Explain what we're doing. Or make them do it within build_rcnn.\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    features = resnet_v1_101_tail(pooled)[0]\n",
    "    \n",
    "features = tf.reduce_mean(features, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('checkpoint/classes.json') as f:\n",
    "    classes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rcnn(features, num_classes):\n",
    "    \"\"\"Run the RCNN layers through the pooled features.\n",
    "\n",
    "    This directly applies a fully-connected layer from `features`\n",
    "    to the two outputs we want: a class score (plus the background\n",
    "    class) and the bounding box resizings (one per class).\n",
    "    \n",
    "    Arguments:\n",
    "        features: pool_size * pool_size\n",
    "        num_classes:\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors: the first being the output of the bbox\n",
    "        resizings `(W * H * proposals, 4)` and the second being\n",
    "        the class scores, of size `(pool_size ^ 2 * proposals,\n",
    "        num_classes)`.\n",
    "    \"\"\"\n",
    "\n",
    "    # See `tf.layers.dense`, `tf.nn.softmax`. Call them `rcnn/fc_classifier`,\n",
    "    # `rcnn_fc_bbox`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return rcnn_bbox, rcnn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    bbox_pred, cls_prob = build_rcnn(features, len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the most probable class for each proposal, before applying the final resizing (this is what the classifier actually looked at)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_classes = ['background'] + classes\n",
    "    \n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "\n",
    "slider = IntSlider(\n",
    "    min=0, max=len(preds) // 15, description='Proposals',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(im_idx=slider)\n",
    "def display_predictions(im_idx=0):\n",
    "    figsize = (15, 9)\n",
    "    fig, axes = plt.subplots(3, 5, figsize=figsize)\n",
    "    \n",
    "    for idx, ax in enumerate(axes.ravel()):        \n",
    "        img = image_crops[im_idx * 15 + idx, :, :, :]\n",
    "        base_image = Image.fromarray(img)\n",
    "        \n",
    "        ax.imshow(base_image, aspect='auto')\n",
    "        ax.set_title(output_classes[preds[im_idx * 15 + idx]])\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now display bbox resizings for these predictions.\n",
    "\n",
    "Corrections are done per-class. We take some proposals and apply the different possible resizings.\n",
    "\n",
    "For each region, we first display the resizing for the most probable class and then for three other random classes. If the most probable class is background, we ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target normalization variances to adjust the output of the R-CNN so it trains better.\n",
    "TARGET_VARIANCES = np.array([0.1, 0.1, 0.2, 0.2], dtype=np.float32)\n",
    "\n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "non_background = (preds != 0)\n",
    "\n",
    "non_bg_preds = preds[non_background]\n",
    "non_bg_proposals = proposals.numpy()[non_background]\n",
    "non_bg_bboxes = bbox_pred.numpy()[non_background]\n",
    "\n",
    "count = len(np.flatnonzero(non_background))\n",
    "\n",
    "slider = IntSlider(\n",
    "    min=0, max=count // 3, description='Proposals',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(page_idx=slider)\n",
    "def display_resizings(page_idx=0):\n",
    "    figsize = (15, 9)\n",
    "    fig, axes = plt.subplots(3, 5, figsize=figsize)\n",
    "    \n",
    "    for row_idx, cols in enumerate(axes):\n",
    "        proposal_idx = page_idx * 3 + row_idx\n",
    "        for col in cols:\n",
    "            col.axis('off')\n",
    "        \n",
    "        # Original region.\n",
    "        # (Using original region size so comparison is easier to the eye.)\n",
    "        # region = Image.fromarray(image_crops[proposal_idx, :, :, :])\n",
    "        x_min, y_min, x_max, y_max = clip_boxes(\n",
    "            non_bg_proposals[proposal_idx:proposal_idx+1],\n",
    "            image.shape[1:3]\n",
    "        )[0]\n",
    "        region = Image.fromarray(\n",
    "            image[0, int(y_min):int(y_max), int(x_min):int(x_max), :]\n",
    "        )\n",
    "        cols[0].imshow(region)\n",
    "        cols[0].set_title('Region')\n",
    "        \n",
    "        # Per-class region, correct class first.\n",
    "        class_ids = np.concatenate([\n",
    "            np.array([non_bg_preds[proposal_idx] - 1]),\n",
    "            np.random.randint(0, len(classes), 3)\n",
    "        ])\n",
    "        for col, class_id in zip(cols[1:], class_ids):\n",
    "            cls_bbox_pred = non_bg_bboxes[\n",
    "                proposal_idx:proposal_idx + 1,\n",
    "                (4 * class_id):(4 * class_id + 4)\n",
    "            ]\n",
    "\n",
    "            cls_objects = decode(\n",
    "                non_bg_proposals[proposal_idx:proposal_idx+1],\n",
    "                cls_bbox_pred * TARGET_VARIANCES\n",
    "            ).numpy()\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = clip_boxes(\n",
    "                cls_objects, image.shape[1:3]\n",
    "            )[0]\n",
    "\n",
    "            region = Image.fromarray(\n",
    "                image[0, int(y_min):int(y_max), int(x_min):int(x_max), :]\n",
    "            )\n",
    "            \n",
    "            col.imshow(region)\n",
    "            col.set_title(classes[class_id])\n",
    "        \n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the object proposals\n",
    "\n",
    "Getting the final predictions.\n",
    "\n",
    "(We're not going to implement this; same as above but on a class-by-class basis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects, labels, probs = rcnn_proposals(\n",
    "    proposals, bbox_pred, cls_prob, image.shape[1:3], 80,\n",
    "    min_prob_threshold=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slider = FloatSlider(\n",
    "    min=0.0, max=1.0, step=0.01, value=0.7,\n",
    "    description='Probability threshold',\n",
    "    layout=Layout(width='600px')\n",
    ")\n",
    "\n",
    "@interact(prob=slider)\n",
    "def display_objects(prob):\n",
    "    MAX_TO_DRAW = 50\n",
    "\n",
    "    mask = probs > prob\n",
    "\n",
    "    return draw_bboxes_with_labels(\n",
    "        image,\n",
    "        classes,\n",
    "        objects.numpy()[mask][:MAX_TO_DRAW],\n",
    "        labels.numpy()[mask][:MAX_TO_DRAW],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing up\n",
    "\n",
    "[what we did]\n",
    "[what else to do? training, FPN]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
